{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd22ed13a50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28), nrow=5, show=True):\n",
    "    image_tensor = (image_tensor + 1) / 2# rescales to from the range [-1, 1] to [0, 1]\n",
    "    image_unflat = image_tensor.detach().cpu() # detached from the computation graph\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze()) # squeeze to remove any singleton dim\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim=10, im_chan=1, hidden_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.gen = nn.Sequential(\n",
    "            self.make_gen_block(input_dim, hidden_dim * 4),\n",
    "            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\n",
    "            self.make_gen_block(hidden_dim * 2, hidden_dim),\n",
    "            self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                # increasing the spatial resolution\n",
    "                # deconvolution, upsampling\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride), # for generating images with higgher dimension\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True), # used only in non-final layers, helps the network lean complex patterns in the data. introduce non-linearity\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.Tanh(), # range [-1, 1] - matches the typical range of real image pixel values normalized to this range.\n",
    "            )\n",
    "\n",
    "    def forward(self, noise): # len(noise) - batch size\n",
    "        x = noise.view(len(noise), self.input_dim, 1, 1) # (1, 1) - row and col\n",
    "        return self.gen(x) \n",
    "\n",
    "def get_noise(n_samples, input_dim, device='cpu'):\n",
    "    return torch.randn(n_samples, input_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, im_chan=1, hidden_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            self.make_disc_block(im_chan, hidden_dim),\n",
    "            self.make_disc_block(hidden_dim, hidden_dim * 2),\n",
    "            self.make_disc_block(hidden_dim * 2, 1, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_disc_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True), # to learn both positive and negtive values in the feature map\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride), # it is binray classification, only one output value, so no activations.\n",
    "            )\n",
    "\n",
    "    def forward(self, image):\n",
    "        disc_pred = self.disc(image)\n",
    "        return disc_pred.view(len(disc_pred), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting one-hot vector for the labels - conditional information\n",
    "def get_one_hot_labels(labels, n_classes):\n",
    "    return F.one_hot(labels, num_classes=n_classes) # returns (?, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining two vectors, noise vecotr and the on-hot vector\n",
    "\n",
    "def combine_vectors(x, y):\n",
    "    combined = torch.cat(tensors=[x, y], dim=1).float()\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_shape = (1, 28, 28)\n",
    "n_classes = 10\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "n_epochs = 200\n",
    "z_dim = 64\n",
    "display_step = 500\n",
    "batch_size = 128\n",
    "lr = 0.0002\n",
    "device = 'cuda'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    MNIST('../W1 Intro to GAN/', download=False, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for getting the size of the conditional input dimension\n",
    "def get_input_dimensions(z_dim, mnist_shape, n_classes):\n",
    "    generator_input_dim = z_dim + n_classes\n",
    "\n",
    "    # Calculate the number of input channels for the discriminator\n",
    "    discriminator_im_chan = mnist_shape[0] + n_classes # channels in MNIST images with n_classes\n",
    "\n",
    "    return generator_input_dim, discriminator_im_chan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_input_dim, discriminator_im_chan = get_input_dimensions(z_dim, mnist_shape, n_classes)\n",
    "\n",
    "gen = Generator(input_dim=generator_input_dim).to(device)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
    "\n",
    "disc = Discriminator(im_chan=discriminator_im_chan).to(device)\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d): # for convolution layers\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d): # for batchnorm layers\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "gen = gen.apply(weights_init)\n",
    "disc = disc.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn cGANs, control over the gen imgs is done by providing conditional information to both the generator and discriminator/ \\n\\nGen takes both random noise and conditionla information as input.\\nDisc takes both real images and generated images along with the corresponding conditional information\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "In cGANs, control over the gen imgs is done by providing conditional information to both the generator and discriminator/ \n",
    "\n",
    "Gen takes both random noise and conditionla information as input.\n",
    "Disc takes both real images and generated images along with the corresponding conditional information\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_real_and_fake_images(real_images, fake_images, num_images=5, size=(1, 28, 28), nrow=5):\n",
    "    \"\"\"\n",
    "    Display real and fake images side by side.\n",
    "\n",
    "    Args:\n",
    "        real_images (torch.Tensor): A tensor containing real images.\n",
    "        fake_images (torch.Tensor): A tensor containing fake/generated images.\n",
    "        num_images (int): The number of images to display for each type (real and fake).\n",
    "        size (tuple): The size of the images (e.g., (1, 28, 28) for grayscale MNIST).\n",
    "        nrow (int): Number of images to display in each row.\n",
    "\n",
    "    \"\"\"\n",
    "    real_images = (real_images + 1) / 2  # Rescale from [-1, 1] to [0, 1]\n",
    "    fake_images = (fake_images + 1) / 2\n",
    "\n",
    "    # Detach from the computation graph and move to CPU for display\n",
    "    real_images_unflat = real_images.detach().cpu()\n",
    "    fake_images_unflat = fake_images.detach().cpu()\n",
    "\n",
    "    # Create image grids for real and fake images\n",
    "    real_image_grid = make_grid(real_images_unflat[:num_images], nrow=nrow)\n",
    "    fake_image_grid = make_grid(fake_images_unflat[:num_images], nrow=nrow)\n",
    "\n",
    "    # Plot real and fake images side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axes[0].imshow(real_image_grid.permute(1, 2, 0).squeeze(), cmap='gray')\n",
    "    axes[0].set_title(\"Real Images\")\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(fake_image_grid.permute(1, 2, 0).squeeze(), cmap='gray')\n",
    "    axes[1].set_title(\"Fake Images\")\n",
    "    axes[1].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_step = 0\n",
    "\n",
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "\n",
    "noise_and_labels = False\n",
    "fake = False\n",
    "\n",
    "fake_image_and_labels = False\n",
    "real_image_and_labels = False\n",
    "\n",
    "disc_fake_pred = False\n",
    "disc_real_pred = False\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for real, labels in tqdm(dataloader):\n",
    "        cur_batch_size = len(real) # no of images in the curr batch\n",
    "        \n",
    "        real = real.to(device)\n",
    "        \n",
    "        one_hot_labels = get_one_hot_labels(labels.to(device), n_classes) # move tensor to the device\n",
    "        # (?, num_classes)\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None] \n",
    "        image_one_hot_labels = image_one_hot_labels.repeat(1, 1, mnist_shape[1],mnist_shape[2]) # reshapes one-hot labels to match the shape of the images in the dataset. it repeats the on-hot labels across the imaeg dimensions\n",
    "        \n",
    "        ### Update the discriminator\n",
    "        disc_opt.zero_grad() # initilize the grads of disc\n",
    "        fake_noise = get_noise(cur_batch_size, z_dim, device) #gens random noise of batch size\n",
    "        \n",
    "        # Combine the noise vectors and the one-hot labels for the generator\n",
    "        noise_and_labels = combine_vectors(fake_noise, one_hot_labels)\n",
    "        \n",
    "        fake = gen(noise_and_labels) # generates fake images\n",
    "        # Generate the conditional Fake images\n",
    "\n",
    "        '''\n",
    "        - Create the input for the disc\n",
    "        - get the disc pred on the fakes as disc_fake_pred\n",
    "        - get the disc pred on the reals as disc_fake_pred\n",
    "        '''\n",
    "        # prepare input for the disc\n",
    "        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels) # combine fake with labels\n",
    "        real_image_and_labels = combine_vectors(real, image_one_hot_labels) # combine real with labels\n",
    "        \n",
    "        '''Training with real and fake images'''\n",
    "        # disc predictions fr the fake and real images\n",
    "        disc_fake_pred = disc(fake_image_and_labels.detach())\n",
    "        disc_real_pred = disc(real_image_and_labels)\n",
    "        \n",
    "        \n",
    "        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "        \n",
    "        disc_loss = (disc_fake_loss + disc_real_loss) / 2 # average loss of these two losses\n",
    "        \n",
    "        disc_loss.backward(retain_graph = True) # update the disc weights\n",
    "        disc_opt.step()\n",
    "        \n",
    "        discriminator_losses += [disc_loss.item()]\n",
    "        \n",
    "        \n",
    "        ### Update the Generator\n",
    "        gen_opt.zero_grad()\n",
    "        \n",
    "\n",
    "        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels) # generated image and it's real lables\n",
    "        disc_fake_pred = disc(fake_image_and_labels) # value of the prediction of disc\n",
    "        \n",
    "        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "        \n",
    "        gen_loss.backward()\n",
    "        gen_opt.step()\n",
    "        \n",
    "        generator_losses += [gen_loss.item()]\n",
    "        \n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            gen_mean = sum(generator_losses[-display_step:]) / display_step\n",
    "            disc_mean = sum(discriminator_losses[-display_step:]) / display_step\n",
    "            \n",
    "            print(f\"Step {cur_step}: Generator loss: {gen_mean}, discriminator loss: {disc_mean}\")\n",
    "\n",
    "            show_tensor_images(fake)\n",
    "            show_tensor_images(real)\n",
    "            \n",
    "            step_bins = 20\n",
    "            \n",
    "            x_axis = sorted([i * step_bins for i in range(len(generator_losses) // step_bins)] * step_bins)\n",
    "            num_examples = (len(generator_losses) // step_bins) * step_bins\n",
    "            \n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins),\n",
    "                torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                label = \"Generated Loss\"\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins),\n",
    "                torch.Tensor(discriminator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                label = \"Discriminator Loss\"\n",
    "            )\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        elif cur_step == 0:\n",
    "            print(\"Training Started......\")\n",
    "        \n",
    "        cur_step += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (gen): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(10, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (1): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = Generator()\n",
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (disc): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(128, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc = Discriminator()\n",
    "disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = gen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (gen): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(10, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (1): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1+cu117'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "n_interpolation = 9 # no of intermediate images we want + 2 (for the start and end image)\n",
    "interpolation_noise = get_noise(1, z_dim, device=device).repeat(n_interpolation, 1)\n",
    "\n",
    "def interpolate_class(first_number, second_number):\n",
    "    first_label = get_one_hot_labels(torch.Tensor([first_number]).long(), n_classes)\n",
    "    second_label = get_one_hot_labels(torch.Tensor([second_number]).long(), n_classes)\n",
    "\n",
    "    # Calculate the interpolation vector between the two labels\n",
    "    percent_second_label = torch.linspace(0, 1, n_interpolation)[:, None]\n",
    "    interpolation_labels = first_label * (1 - percent_second_label) + second_label * percent_second_label\n",
    "\n",
    "    # Combine the noise and the labels\n",
    "    noise_and_labels = combine_vectors(interpolation_noise, interpolation_labels.to(device))\n",
    "    fake = gen(noise_and_labels)\n",
    "    show_tensor_images(fake, num_images=n_interpolation, nrow=int(math.sqrt(n_interpolation)), show=False)\n",
    "\n",
    "start_plot_number = 1 #  start digit\n",
    "\n",
    "end_plot_number = 5 #   end digit\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "interpolate_class(start_plot_number, end_plot_number)\n",
    "_ = plt.axis('off')\n",
    "\n",
    "\n",
    "plot_numbers = [2, 3, 4, 5, 7]\n",
    "n_numbers = len(plot_numbers)\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i, first_plot_number in enumerate(plot_numbers):\n",
    "    for j, second_plot_number in enumerate(plot_numbers):\n",
    "        plt.subplot(n_numbers, n_numbers, i * n_numbers + j + 1)\n",
    "        interpolate_class(first_plot_number, second_plot_number)\n",
    "        plt.axis('off')\n",
    "plt.subplots_adjust(top=1, bottom=0, left=0, right=1, hspace=0.1, wspace=0)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_interpolation = 9 # How many intermediate images you want + 2 (for the start and end image)\n",
    "\n",
    "# This time you're interpolating between the noise instead of the labels\n",
    "interpolation_label = get_one_hot_labels(torch.Tensor([5]).long(), n_classes).repeat(n_interpolation, 1).float()\n",
    "\n",
    "def interpolate_noise(first_noise, second_noise):\n",
    "    # This time you're interpolating between the noise instead of the labels\n",
    "    percent_first_noise = torch.linspace(0, 1, n_interpolation)[:, None].to(device)\n",
    "    interpolation_noise = first_noise * percent_first_noise + second_noise * (1 - percent_first_noise)\n",
    "\n",
    "    # Combine the noise and the labels again\n",
    "    noise_and_labels = combine_vectors(interpolation_noise, interpolation_label.to(device))\n",
    "    fake = gen(noise_and_labels)\n",
    "    show_tensor_images(fake, num_images=n_interpolation, nrow=int(math.sqrt(n_interpolation)), show=False)\n",
    "\n",
    "# Generate noise vectors to interpolate between\n",
    "### Change me! ###\n",
    "n_noise = 5 # Choose the number of noise examples in the grid\n",
    "plot_noises = [get_noise(1, z_dim, device=device) for i in range(n_noise)]\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i, first_plot_noise in enumerate(plot_noises):\n",
    "    for j, second_plot_noise in enumerate(plot_noises):\n",
    "        plt.subplot(n_noise, n_noise, i * n_noise + j + 1)\n",
    "        interpolate_noise(first_plot_noise, second_plot_noise)\n",
    "        plt.axis('off')\n",
    "plt.subplots_adjust(top=1, bottom=0, left=0, right=1, hspace=0.1, wspace=0)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
